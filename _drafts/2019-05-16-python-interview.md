---
layout: post
title:  " Another Python interview questions "
author: tony
categories: [ Python, Interview ]
image: assets/images/pythonfeatured.jpg
tags: [ python, interview ]
---




1. 大數據的文件讀取:
    - 利用生成器generator
    - 迭代器進行迭代遍歷：for line in file

2. 迭代器和生成器的區別：
    1. 迭代器是一個更抽象的概念，任何對象，如果它的類有next方法和iter方法返回自己本身。對於string、list、dict、tuple等這類容器對象，使用for循環遍歷是很方便的。在後臺for語句對容器對象調用iter()函數，iter()是python的內置函數。iter()會返回一個定義了next()方法的迭代器對象，它在容器中逐個訪問容器內元素，next()也是python的內置函數。在沒有後續元素時，next()會拋出一個StopIteration異常   
    2. 生成器（Generator）是創建迭代器的簡單而強大的工具。它們寫起來就像是正規的函數，只是在需要返回數據的時候使用yield語句。每次next()被調用時，生成器會返回它脫離的位置（它記憶語句最後一次執行的位置和所有的數據值）  
區別：生成器能做到迭代器能做的所有事,而且因為自動創建了__iter__()和next()方法,生成器顯得特別簡潔,而且生成器也是高效的，使用生成器表達式取代列表解析可以同時節省內存。除了創建和保存程序狀態的自動方法,當發生器終結時,還會自動拋出StopIteration異常

3. 裝飾器的作用和功能：  
    引入日誌；函數執行時間統計；執行函數前預備處理；執行函數後的清理功能；權限校驗等場景；緩存；

4. 談談你對異步阻塞非阻塞的理解：  
略

5. 簡單談下GIL：  
 Global Interpreter Lock(全局解釋器鎖)
    Python代碼的執行由Python 虛擬機(也叫解釋器主循環，CPython版本)來控制，Python 在設計之初就考慮到要在解釋器的主循環中，同時只有一個線程在執行，即在任意時刻，只有一個線程在解釋器中運行。對Python 虛擬機的訪問由全局解釋器鎖（GIL）來控制，正是這個鎖能保證同一時刻只有一個線程在運行。
在多線程環境中，Python 虛擬機按以下方式執行：
    1. 設置GIL
    2. 切換到一個線程去運行
    3. 運行：  
        a. 指定數量的字節碼指令，或者  
        b. 線程主動讓出控制（可以調用time.sleep(0)）
    4. 把線程設置為睡眠狀態
    5. 解鎖GIL
    6. 再次重複以上所有步驟  
在調用外部代碼（如C/C++擴展函數）的時候，GIL將會被鎖定，直到這個函數結束為止（由於在這期間沒有Python 的字節碼被運行，所以不會做線程切換）。

6. 簡單談下python2和python3的區別：  

7. find和grep?：  
grep命令是一種強大的文本搜索工具，grep搜索內容串可以是正則表達式，允許對文本文件進行模式查找。如果找到匹配模式，grep打印包含模式的所有行。  
find通常用來再特定的目錄下搜索符合條件的文件，也可以用來搜索特定用戶屬主的文件。

8. 線上服務可能因為種種原因導致掛掉怎麽辦？  
    - linux下的後臺進程管理利器 supervisor
    - 每次文件修改後再linux執行 service supervisord restart

9. 如何提高python的運行效率  
使用生成器；關鍵代碼使用外部功能包（Cython，pylnlne，pypy，pyrex）；針對循環的優化--盡量避免在循環中訪問變量的屬性

10. 常用Linux命令：  
ls,help,cd,more,clear,mkdir,pwd,rm,grep,find,mv,su,date

11. python簡單的列表去重  
set() 集合去重

12. python 中 yield 的用法？  
yield簡單說來就是一個生成器，這樣函數它記住上次返回時在函數體中的位置。對生成器第二次（或n 次）調用跳轉至該函 次）調用跳轉至該函數。

13. python是如何進行內存管理的？    
    a. 垃圾回收：python不像C++，Java等語言一樣，他們可以不用事先聲明變量類型而直接對變量進行賦值。對Python語言來講，對象的類型和內存都是在運行時確定的。這也是為什麽我們稱Python語言為動態類型的原因（這里我們把動態類型可以簡單的歸結為對變量內存地址的分配是在運行時自動判斷變量類型並對變量進行賦值）。   
    b. 引用計數：Python采用了類似Windows內核對象一樣的方式來對內存進行管理。每一個對象，都維護這一個對指向該對對象的引用的計數。當變量被綁定在一個對象上的時候，該變量的引用計數就是1，(還有另外一些情況也會導致變量引用計數的增加),系統會自動維護這些標簽，並定時掃描，當某標簽的引用計數變為0的時候，該對就會被回收。  
    c. 內存池機制Python的內存機制以金字塔行，-1，-2層主要有操作系統進行操作， 第0層是C中的malloc，free等內存分配和釋放函數進行操作；第1層和第2層是內存池，有Python的接口函數PyMem_Malloc函數實現，當對象小於256K時有該層直接分配內存；第3層是最上層，也就是我們對Python對象的直接操作；在 C 中如果頻繁的調用 malloc 與 free 時,是會產生性能問題的.再加上頻繁的分配與釋放小塊的內存會產生內存碎片. Python 在這里主要幹的工作有:如果請求分配的內存在1~256字節之間就使用自己的內存管理系統,否則直接使用 malloc.這里還是會調用 malloc 分配內存,但每次會分配一塊大小為256k的大塊內存.經由內存池登記的內存到最後還是會回收到內存池,並不會調用 C 的 free釋放掉.以便下次使用.對於簡單的Python對象，例如數值、字符串，元組（tuple不允許被更改)采用的是複制的方式(深拷貝?)，也就是說當將另一個變量B賦值給變量A時，雖然A和B的內存空間仍然相同，但當A的值發生變化時，會重新給A分配空間，A和B的地址變得不再相同    

14. 描述數組、鏈表、隊列、堆棧的區別？
    數組與鏈表是數據存儲方式的概念，數組在連續的空間中存儲數據，而鏈表可以在非連續的空間中存儲數據；
    隊列和堆棧是描述數據存取方式的概念，隊列是先進先出，而堆棧是後進先出；隊列和堆棧可以用數組來實現，也可以用鏈表實現。 

15. 你知道幾種排序,講一講你最熟悉的一種?  


## Web框架部分
1. django 中當一個用戶登錄 A 應用服務器（進入登錄狀態），然後下次請求被 nginx 代理到 B 應用服務器會出現什麽影響？  
如果用戶在A應用服務器登陸的session數據沒有共享到B應用服務器，納米之前的登錄狀態就沒有了。

2. 跨域請求問題django怎麽解決的（原理）
    - 啟用中間件
    - post請求
    - 驗證碼
    - 表單中添加{%csrf_token%}標簽

3. 請解釋或描述一下Django的架構  
    對於Django框架遵循MVC設計，並且有一個專有名詞：MVT  
    M全拼為Model，與MVC中的M功能相同，負責數據處理，內嵌了ORM框架  
    V全拼為View，與MVC中的C功能相同，接收HttpRequest，業務處理，返回HttpResponse  
    T全拼為Template，與MVC中的V功能相同，負責封裝構造要返回的html，內嵌了模板引擎  
    
4. django對數據查詢結果排序怎麽做，降序怎麽做，查詢大於某個字段怎麽做  
    - 排序使用order_by()  
    - 降序需要在排序字段名前加-  
    - 查詢字段大於某個值：使用filter(字段名_gt=值)  

5. 說一下Django，MIDDLEWARES中間件的作用？  
中間件是介於request與response處理之間的一道處理過程，相對比較輕量級，並且在全局上改變django的輸入與輸出。

6. 你對Django的認識？  
Django是走大而全的方向，它最出名的是其全自動化的管理後臺：只需要使用起ORM，做簡單的對象定義，它就能自動生成數據庫結構、以及全功能的管理後臺。
Django內置的ORM跟框架內的其他模塊耦合程度高。  
應用程序必須使用Django內置的ORM，否則就不能享受到框架內提供的種種基於其ORM的便利；理論上可以切換掉其ORM模塊，但這就相當於要把裝修完畢的房子拆除重新裝修，倒不如一開始就去毛胚房做全新的裝修。  
Django的賣點是超高的開發效率，其性能擴展有限；采用Django的項目，在流量達到一定規模後，都需要對其進行重構，才能滿足性能的要求。  
Django適用的是中小型的網站，或者是作為大型網站快速實現產品雛形的工具。  
Django模板的設計哲學是徹底的將代碼、樣式分離； Django從根本上杜絕在模板中進行編碼、處理數據的可能。

7. Django重定向你是如何實現的？用的什麽狀態碼？
    - 使用HttpResponseRedirect
    - redirect和reverse
    - 狀態碼：302,301
    
8. ngnix的正向代理與反向代理？  
    - 正向代理 是一個位於客戶端和原始服務器(origin server)之間的服務器，為了從原始服務器取得內容，客戶端向代理發送一個請求並指定目標(原始服務器)，然後代理向原始服務器轉交請求並將獲得的內容返回給客戶端。客戶端必須要進行一些特別的設置才能使用正向代理。
    - 反向代理正好相反，對於客戶端而言它就像是原始服務器，並且客戶端不需要進行任何特別的設置。客戶端向反向代理的命名空間中的內容發送普通請求，接著反向代理將判斷向何處(原始服務器)轉交請求，並將獲得的內容返回給客戶端，就像這些內容原本就是它自己的一樣。

9. Tornado 的核是什麽？  
Tornado 的核心是 ioloop 和 iostream 這兩個模塊，前者提供了一個高效的 I/O 事件循環，後者則封裝了 一個無阻塞的 socket 。通過向 ioloop 中添加網絡 I/O 事件，利用無阻塞的 socket ，再搭配相應的回調 函數，便可達到夢寐以求的高效異步執行。

10.Django 本身提供了 runserver，為什麽不能用來部署？  

runserver 方法是調試 Django 時經常用到的運行方式，它使用 Django 自帶的  
WSGI Server 運行，主要在測試和開發中使用，並且 runserver 開啟的方式也是單進程 。  
uWSGI 是一個 Web 服務器，它實現了 WSGI 協議、uwsgi、http 等協議。註意 uwsgi 是一種通信協議，而 uWSGI 是實現 uwsgi 協議和 WSGI 協議的 Web 服務器。uWSGI 具有超快的性能、低內存占用和多 app 管理等優點，並且搭配著 Nginx
就是一個生產環境了，能夠將用戶訪問請求與應用 app 隔離開，實現真正的部署 。相比來講，支持的並發量更高，方便管理多進程，發揮多核的優勢，提升性能。

## 網絡編程和前端部分
1. AJAX是什麽，如何使用AJAX？  
ajax(異步的javascript 和xml) 能夠刷新局部網頁數據而不是重新加載整個網頁。  
Step1，創建xmlhttprequest對象，var xmlhttp =new XMLHttpRequest（);XMLHttpRequest對象用來和服務器交換數據。  
Step2，使用xmlhttprequest對象的open（）和send（）方法發送資源請求給服務器。  
Step3，使用xmlhttprequest對象的responseText或responseXML屬性獲得服務器的響應。  
Step4，onreadystatechange函數，當發送請求到服務器，我們想要服務器響應執行一些功能就需要使用onreadystatechange函數，每次xmlhttprequest對象的readyState發生改變都會觸發onreadystatechange函數。
2. 常見的HTTP狀態碼有哪些？  
    - 200 OK
    - 301 Moved Permanently
    - 302 Found
    - 304 Not Modified
    - 307 Temporary Redirect
    - 400 Bad Request
    - 401 Unauthorized
    - 403 Forbidden
    - 404 Not Found
    - 410 Gone
    - 500 Internal Server Error
    - 501 Not Implemented
3. Post和get區別？
    1) GET請求，請求的數據會附加在URL之後，以?分割URL和傳輸數據，多個參數用&連接。URL的編碼格式采用的是ASCII編碼，而不是uniclde，即是說所有的非ASCII字符都要編碼之後再傳輸。  
    POST請求：POST請求會把請求的數據放置在HTTP請求包的包體中。上面的item=bandsaw就是實際的傳輸數據。  
    因此，GET請求的數據會暴露在地址欄中，而POST請求則不會。  
    2) 傳輸數據的大小
    在HTTP規範中，沒有對URL的長度和傳輸的數據大小進行限制。但是在實際開發過程中，對於GET，特定的瀏覽器和服務器對URL的長度有限制。因此，在使用GET請求時，傳輸數據會受到URL長度的限制。  
    對於POST，由於不是URL傳值，理論上是不會受限制的，但是實際上各個服務器會規定對POST提交數據大小進行限制，Apache、IIS都有各自的配置。  
    3) 安全性  
    POST的安全性比GET的高。這里的安全是指真正的安全，而不同於上面GET提到的安全方法中的安全，上面提到的安全僅僅是不修改服務器的數據。比如，在進行登錄操作，通過GET請求，用戶名和密碼都會暴露再URL上，因為登錄頁面有可能被瀏覽器緩存以及其他人查看瀏覽器的歷史記錄的原因，此時的用戶名和密碼就很容易被他人拿到了。除此之外，GET請求提交的數據還可能會造成Cross-site request frogery攻擊。

4. cookie 和session 的區別？

    1) cookie數據存放在客戶的瀏覽器上，session數據放在服務器上。
    2) cookie不是很安全，別人可以分析存放在本地的COOKIE並進行COOKIE欺騙考慮到安全應當使用session。
    3) session會在一定時間內保存在服務器上。當訪問增多，會比較占用服務器的性能考慮到減輕服務器性能方面，應當使用COOKIE。
    4) 單個cookie保存的數據不能超過4K，很多瀏覽器都限制一個站點最多保存20個cookie。
    5) 建議：  
將登陸信息等重要信息存放為SESSION
       其他信息如果需要保留，可以放在COOKIE中

5.創建一個簡單tcp服務器需要的流程
1. socket創建一個套接字
2. bind綁定ip和port
3. listen使套接字變為可以被動鏈接
4. accept等待客戶端的鏈接
5. recv/send接收發送數據
6. 請簡單說一下三次握手和四次揮手？什麽是2msl？為什麽要這樣做？  
2MSL即兩倍的MSL，TCP的TIME_WAIT狀態也稱為2MSL等待狀態，當TCP的一端發起主動關閉，在發出最後一個ACK包後，即第3次握手完成後發送了第四次握手的ACK包後就進入了TIME_WAIT狀態，必須在此狀態上停留兩倍的MSL時間，等待2MSL時間主要目的是怕最後一個 ACK包對方沒收到，那麽對方在超時後將重發第三次握手的FIN包，主動關閉端接到重發的FIN包後可以再發一個ACK應答包。  
在TIME_WAIT狀態 時兩端的端口不能使用，要等到2MSL時間結束才可繼續使用。當連接處於2MSL等待階段時任何遲到的報文段都將被丟棄。不過在實際應用中可以通過設置SO_REUSEADDR選項達到不必等待2MSL時間結束再使用此端口。  

## 爬蟲和數據庫部分
1. scrapy和scrapy-redis有什麽區別？為什麽選擇redis數據庫？
    1) scrapy是一個Python爬蟲框架，爬取效率極高，具有高度定制性，但是不支持分布式。而scrapy-redis一套基於redis數據庫、運行在scrapy框架之上的組件，可以讓scrapy支持分布式策略，Slaver端共享Master端redis數據庫里的item隊列、請求隊列和請求指紋集合。
    2) 為什麽選擇redis數據庫，因為redis支持主從同步，而且數據都是緩存在內存中的，所以基於redis的分布式爬蟲，對請求和數據的高頻讀取效率非常高。
2. 你用過的爬蟲框架或者模塊有哪些？談談他們的區別或者優缺點？  
    - Python自帶：urllib，urllib2
    - 第 三 方：requests
    - 框    架：Scrapy  
urllib和urllib2模塊都做與請求URL相關的操作，但他們提供不同的功能。  
urllib2.：urllib2.urlopen可以接受一個Request對象或者url，（在接受Request對象時候，並以此可以來設置一個URL 的headers），urllib.urlopen只接收一個url  
urllib 有urlencode,urllib2沒有，因此總是urllib，urllib2常會一起使用的原因  
scrapy是封裝起來的框架，他包含了下載器，解析器，日誌及異常處理，基於多線程， twisted的方式處理，對於固定單個網站的爬取開發，有優勢，但是對於多網站爬取 100個網站，並發及分布式處理方面，不夠靈活，不便調整與括展。  
request 是一個HTTP庫，它只是用來，進行請求，對於HTTP請求，他是一個強大的庫，下載，解析全部自己處理，靈活性更高，高並發與分布式部署也非常靈活，對於功能可以更好實現  

    Scrapy優缺點：  
        - 優點：scrapy 是異步的  
        采取可讀性更強的xpath代替正則  
        強大的統計和log系統  
        同時在不同的url上爬行  
        支持shell方式，方便獨立調試  
        寫middleware,方便寫一些統一的過濾器  
        通過管道的方式存入數據庫  
        - 缺點：基於python的爬蟲框架，擴展性比較差    
        基於twisted框架，運行中的exception是不會幹掉reactor，並且異步框架出錯後是不會停掉其他任務的，數據出錯後難以察覺。  

3.你常用的mysql引擎有哪些？各引擎間有什麽區別?  
    主要 MyISAM 與 InnoDB 兩個引擎，其主要區別如下：  
    1) InnoDB 支持事務，MyISAM不支持，這一點是非常之重要。事務是一種高級的處理方式，如在一些列增刪改中只要哪個出錯還可以回滾還原，而 MyISAM就不可以了
    2) MyISAM 適合查詢以及插入為主的應用，InnoDB 適合頻繁修改以及涉及到安全性較高的應用
    3) InnoDB 支持外鍵，MyISAM 不支持
    4) MyISAM 是默認引擎，InnoDB 需要指定
    5) InnoDB 不支持 FULLTEXT 類型的索引
    6) InnoDB 中不保存表的行數，如 select count(\*) from table 時，InnoDB；需要掃描一遍整個表來計算有多少行，但是MyISAM只要簡單的讀出保存好的行數即可。註意的是，當 count(*)語句包含 where 條件時 MyISAM 也需要掃描整個表
    7) 對於自增長的字段，InnoDB中必須包含只有該字段的索引，但是在MyISAM表中可以和其他字段一起建立聯合索引
    8) 清空整個表時，InnoDB 是一行一行的刪除，效率非常慢。MyISAM 則會重建表；
    9) InnoDB 支持行鎖（某些情況下還是鎖整表，如 update table set a=1 where user like '%lee%'

4. 描述下scrapy框架運行的機制？  
答：從start_urls里獲取第一批url並發送請求，請求由引擎交給調度器入請求隊列，獲取完畢後，調度器將請求隊列里的請求交給下載器去獲取請求對應的響應資源，並將響應交給自己編寫的解析方法做提取處理：
    1) 如果提取出需要的數據，則交給管道文件處理
    2) 如果提取出url，則繼續執行之前的步驟（發送url請求，並由引擎將請求交給調度器入隊列...)，直到請求隊列里沒有請求，程序結束。

5.什麽是關聯查詢，有哪些？  
答：將多個表聯合起來進行查詢，主要有內連接、左連接、右連接、全連接（外連接）

6.寫爬蟲是用多進程好？還是多線程好？ 為什麽？  
答：IO密集型代碼(文件處理、網絡爬蟲等)，多線程能夠有效提升效率(單線程下有IO操作會進行IO等待，造成不必要的時間浪費，而開啟多線程能在線程A等待時，自動切換到線程B，可以不浪費CPU的資源，從而能提升程序執行效率)。在實際的數據采集過程中，既考慮網速和響應的問題，也需要考慮自身機器的硬件情況，來設置多進程或多線程

7.數據庫的優化？  
    1. 優化索引、SQL 語句、分析慢查詢  
    2. 設計表的時候嚴格根據數據庫的設計範式來設計數據庫  
    3. 使用緩存，把經常訪問到的數據而且不需要經常變化的數據放在緩存中，能節約磁盤IO  
    4. 優化硬件；采用SSD，使用磁盤隊列技術(RAID0,RAID1,RDID5)等  
    5. 采用MySQL 內部自帶的表分區技術，把數據分層不同的文件，能夠提高磁盤的讀取效率  
    6. 垂直分表；把一些不經常讀的數據放在一張表里，節約磁盤I/O  
    7. 主從分離讀寫；采用主從複制把數據庫的讀操作和寫入操作分離開來  
    8. 分庫分表分機器（數據量特別大），主要的的原理就是數據路由  
    9. 選擇合適的表引擎，參數上的優化  
    10. 進行架構級別的緩存，靜態化和分布式  
    11. 不采用全文索引  
    12. 采用更快的存儲方式，例如 NoSQL存儲經常訪問的數據  

8. 常見的反爬蟲和應對方法？

    1.) 通過Headers反爬蟲
    
    從用戶請求的Headers反爬蟲是最常見的反爬蟲策略。很多網站都會對Headers的User-Agent進行檢測，還有一部分網站會對Referer進行檢測（一些資源網站的防盜鏈就是檢測Referer）。如果遇到了這類反爬蟲機制，可以直接在爬蟲中添加Headers，將瀏覽器的User-Agent複制到爬蟲的Headers中；或者將Referer值修改為目標網站域名。對於檢測Headers的反爬蟲，在爬蟲中修改或者添加Headers就能很好的繞過。
    
    2.) 基於用戶行為反爬蟲
    
    還有一部分網站是通過檢測用戶行為，例如同一IP短時間內多次訪問同一頁面，或者同一賬戶短時間內多次進行相同操作。
    
    大多數網站都是前一種情況，對於這種情況，使用IP代理就可以解決。可以專門寫一個爬蟲，爬取網上公開的代理ip，檢測後全部保存起來。這樣的代理ip爬蟲經常會用到，最好自己準備一個。有了大量代理ip後可以每請求幾次更換一個ip，這在requests或者urllib2中很容易做到，這樣就能很容易的繞過第一種反爬蟲。
    
    對於第二種情況，可以在每次請求後隨機間隔幾秒再進行下一次請求。有些有邏輯漏洞的網站，可以通過請求幾次，退出登錄，重新登錄，繼續請求來繞過同一賬號短時間內不能多次進行相同請求的限制。
    
    3.) 動態頁面的反爬蟲
    
    上述的幾種情況大多都是出現在靜態頁面，還有一部分網站，我們需要爬取的數據是通過ajax請求得到，或者通過JavaScript生成的。首先用Fiddler對網絡請求進行分析。如果能夠找到ajax請求，也能分析出具體的參數和響應的具體含義，我們就能采用上面的方法，直接利用requests或者urllib2模擬ajax請求，對響應的json進行分析得到需要的數據。
    
    能夠直接模擬ajax請求獲取數據固然是極好的，但是有些網站把ajax請求的所有參數全部加密了。我們根本沒辦法構造自己所需要的數據的請求。這種情況下就用selenium+phantomJS，調用瀏覽器內核，並利用phantomJS執行js來模擬人為操作以及觸發頁面中的js腳本。從填寫表單到點擊按鈕再到滾動頁面，全部都可以模擬，不考慮具體的請求和響應過程，只是完完整整的把人瀏覽頁面獲取數據的過程模擬一遍。
    
    用這套框架幾乎能繞過大多數的反爬蟲，因為它不是在偽裝成瀏覽器來獲取數據（上述的通過添加 Headers一定程度上就是為了偽裝成瀏覽器），它本身就是瀏覽器，phantomJS就是一個沒有界面的瀏覽器，只是操控這個瀏覽器的不是人。利selenium+phantomJS能幹很多事情，例如識別點觸式（12306）或者滑動式的驗證碼，對頁面表單進行暴力破解等。

9. 分布式爬蟲主要解決什麽問題？
    - ip
    - 帶寬
    - cpu
    - io

10. 爬蟲過程中驗證碼怎麽處理？
    1. scrapy自帶（但是成功率不高）
    
    2. 付費接口（若快http://www.ruokuai.com/home/pricetype）
